---
title: "Compare growth models"
author: 'eleanorjackson'
date: '`r format(Sys.time(), "%d %B, %Y")`' 
format: gfm+emoji
editor: source
execute:
  warning: false
---

```{r setup}
#| include: false

file_name <- knitr::current_input()

knitr::opts_chunk$set(
  fig.path =
    paste0("figures/", sub("\\.rmarkdown$", "", basename(file_name)), 
           "/", sep = "")
)

set.seed(123)
ggplot2::theme_set(ggplot2::theme_bw(base_size = 10))
```


```{r}
#| output: FALSE

library("tidyverse")
library("here")
library("patchwork")
library("brms")
library("broom.mixed")
```

```{r}
ft_sp_lognorm <-
  readRDS(here::here("output", "models", "ft_sp_lognorm.rds"))

ft_sp_lognorm_priors <-
  readRDS(here::here("output", "models", "ft_sp_lognorm_priors.rds"))

ft_sp_pl_lognorm_priors <-
  readRDS(here::here("output", "models", "ft_sp_pl_lognorm_priors.rds"))

```

Leave-one-out cross-validation (LOO-CV) 
is a popular method for comparing Bayesian models 
based on their estimated predictive performance on new/unseen data.

Expected log predictive density (ELPD): 
If new observations are well-accounted by the posterior predictive distribution,
then the density of the posterior predictive distribution is high 
and so is its logarithm. 
So higher ELPD = better predictive performance.

```{r}
comp <- loo_compare(ft_sp_lognorm,
                    ft_sp_lognorm_priors,
                    ft_sp_pl_lognorm_priors)

print(comp, digits = 3)
```

```{r}
comp %>% 
  data.frame() %>% 
  rownames_to_column(var = "model_name") %>% 
  ggplot(aes(x    = reorder(model_name, elpd_diff), 
             y    = elpd_diff, 
             ymin = elpd_diff - se_diff, 
             ymax = elpd_diff + se_diff)) +
  geom_pointrange(shape = 21, fill = "white") +
  coord_flip() +
  geom_hline(yintercept = 0, colour = "blue", linetype = 2) +
  labs(x = NULL, y = "difference from model with the largest ELPD", 
       title = "expected log predictive density (ELPD)") 
```

In the [loo package documentation](https://mc-stan.org/loo/articles/online-only/faq.html#how-to-use-cross-validation-for-model-selection-)
they say:

> If elpd difference (`elpd_diff` in loo package) is less than 4, 
the difference is small 
[(Sivula, Magnusson and Vehtari, 2020)](https://doi.org/10.48550/arXiv.2008.10296)). 
If elpd difference is larger than 4,
then compare that difference to standard error of `elpd_diff` 
(provided e.g. by loo package) [(Sivula, Magnusson and Vehtari, 2020)](https://doi.org/10.48550/arXiv.2008.10296).

## Compare parameter estimates

```{r}
my_coef_tab <-
  tibble(model = c(
    "ft_sp_pl_lognorm_priors",
    "ft_sp_lognorm_priors",
    "ft_sp_lognorm"
  )) %>%
  mutate(fit = purrr::map(model, get)) %>%
  mutate(tidy = purrr::map(
    fit,
    tidy,
    parameters = c(
      "b_A_forest_typeprimary",
      "b_A_forest_typesecondary",
      "b_k_forest_typeprimary",
      "b_k_forest_typesecondary",
      "b_delay_forest_typeprimary",
      "b_delay_forest_typesecondary"
    )
  )) %>%
  unnest(tidy)

```

```{r}
#| fig_width: 10
#| fig_height: 10

my_coef_tab %>% 
  ggplot(aes(x = model, y = estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange(shape = 21, fill = "white") +
  labs(x = NULL,
       y = NULL) +
  geom_hline(yintercept = 0,  color = "blue") +
  coord_flip() +
  theme_classic() +
  facet_wrap(~term, 
             ncol = 1,
             scales = "free_x")
```

